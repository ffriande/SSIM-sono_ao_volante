SVM
1st try

'kernel': ['rbf','poly','sigmoid],
'class_weight':['balanced']

results all with rbf = to the ones already obtained (0.79)

2nd try
'C': [1,10,100,1000],
'kernel': ['rbf'],
'class_weight':['balanced']

results all with C=1000 and results 0.1 better than the obtained by diss (0.80)

3rd try
'C': [1000],
'gamma': [1, 0.1, 0.01, 0.001,'scale','auto'],
'kernel': ['rbf'],
'class_weight':[None,'balanced']

results all with gamma=1 and class_weight=None, making results 0.7 better than the obtained by diss (0.86) accuracy
Final results 
Results...
- Accuracy = 0.86 (0.02)
- Kappa = 0.64 (0.03)
- Confusion matrix (mean; std)
[[0.88 0.05 0.08]
 [0.16 0.79 0.05]
 [0.17 0.02 0.8 ]]
[[0.02 0.01 0.01]
 [0.07 0.06 0.04]
 [0.03 0.02 0.03]]
- Stages recall
-- NREM = 0.88 (0.02)
-- REM = 0.79 (0.06)
-- Wake = 0.80 (0.03)


So final parameters are C=1000, gamma=1, kernel='rbf', class_weight='None'
Point: When running with a different dataset, it is better to try all the steps above.


'n_neighbors':[3, 5, 11, 15, 19],
'weights':['uniform', 'distance'],
'metric':['euclidean', 'manhattan']


LIGHTGBM 
DEFAULT
- Accuracy = 0.89 (0.01)
- Kappa = 0.70 (0.03)
- Confusion matrix (mean; std)
[[0.9  0.03 0.07]
 [0.16 0.78 0.06]
 [0.12 0.01 0.87]]
[[0.01 0.01 0.01]
 [0.06 0.07 0.04]
 [0.03 0.01 0.02]]
- Stages recall
-- NREM = 0.90 (0.01)
-- REM = 0.78 (0.07)
-- Wake = 0.87 (0.02)

First Combination

'LGBM__boosting_type': 'gbdt', 'LGBM__max_depth': 5, 'LGBM__min_child_samples': 100, 'LGBM__num_leaves': 21, 'LGBM__objective': 'binary'
- Accuracy = 0.85 (0.01)
- Kappa = 0.63 (0.02)
- Confusion matrix (mean; std)
[[0.85 0.05 0.1 ]
 [0.15 0.76 0.09]
 [0.12 0.02 0.86]]
[[0.01 0.01 0.01]
 [0.05 0.06 0.04]
 [0.04 0.01 0.04]]
- Stages recall
-- NREM = 0.85 (0.01)
-- REM = 0.76 (0.06)
-- Wake = 0.86 (0.04)

Second Combination

'LGBM__boosting_type': 'gbdt', 'LGBM__max_depth': 7, 'LGBM__min_child_samples': 100, 'LGBM__num_leaves': 30, 'LGBM__objective': 'binary'
- Accuracy = 0.88 (0.01)
- Kappa = 0.68 (0.03)
- Confusion matrix (mean; std)
[[0.88 0.04 0.08]
 [0.15 0.78 0.07]
 [0.11 0.02 0.87]]
[[0.01 0.01 0.01]
 [0.03 0.06 0.06]
 [0.02 0.01 0.02]]
- Stages recall
-- NREM = 0.88 (0.01)
-- REM = 0.78 (0.06)
-- Wake = 0.87 (0.02)

Third Combination

'LGBM__boosting_type': 'gbdt', 'LGBM__max_depth': 9, 'LGBM__min_child_samples': 100, 'LGBM__num_leaves': 250, 'LGBM__objective': 'binary'
- Accuracy = 0.89 (0.01)
- Kappa = 0.70 (0.03)
- Confusion matrix (mean; std)
[[0.9  0.03 0.07]
 [0.17 0.77 0.06]
 [0.12 0.01 0.87]]
[[0.01 0.01 0.01]
 [0.05 0.06 0.04]
 [0.02 0.01 0.02]]
- Stages recall
-- NREM = 0.90 (0.01)
-- REM = 0.77 (0.06)
-- Wake = 0.87 (0.02)

Fourth Combination

'LGBM__bagging_fraction': 0.82, 'LGBM__bagging_freq': 1, 'LGBM__boosting_type': 'rf', 'LGBM__max_depth': 9, 'LGBM__min_child_samples': 100, 'LGBM__num_leaves': 110, 'LGBM__objective': 'binary'
- Accuracy = 0.79 (0.02)
- Kappa = 0.53 (0.04)
- Confusion matrix (mean; std)
[[0.78 0.08 0.14]
 [0.11 0.78 0.12]
 [0.12 0.04 0.84]]
[[0.02 0.01 0.02]
 [0.06 0.07 0.07]
 [0.03 0.02 0.04]]
- Stages recall
-- NREM = 0.78 (0.02)
-- REM = 0.78 (0.07)
-- Wake = 0.84 (0.04)

XGBOOST

First Combination

'XGB__colsample_bytree': 1.0, 'XGB__gamma': 0.5, 'XGB__max_depth': 5, 'XGB__min_child_weight': 1, 'XGB__subsample': 0.8
               precision    recall  f1-score   support

        NREM       0.97      0.92      0.95       599
         Rem       0.57      0.73      0.64        37
        Wake       0.76      0.87      0.81       114

    accuracy                           0.91       750
   macro avg       0.77      0.84      0.80       750
weighted avg       0.92      0.91      0.91       750


Results...
- Accuracy = 0.90 (0.01)
- Kappa = 0.71 (0.03)
- Confusion matrix (mean; std)
[[0.91 0.03 0.06]
 [0.19 0.75 0.05]
 [0.13 0.02 0.85]]
[[0.01 0.01 0.01]
 [0.06 0.07 0.04]
 [0.04 0.01 0.03]]
- Stages recall
-- NREM = 0.91 (0.01)
-- REM = 0.75 (0.07)
-- Wake = 0.85 (0.03)
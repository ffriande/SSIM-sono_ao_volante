LIGHTGBM 
DEFAULT
- Accuracy = 0.89 (0.01)
- Kappa = 0.70 (0.03)
- Confusion matrix (mean; std)
[[0.9  0.03 0.07]
 [0.16 0.78 0.06]
 [0.12 0.01 0.87]]
[[0.01 0.01 0.01]
 [0.06 0.07 0.04]
 [0.03 0.01 0.02]]
- Stages recall
-- NREM = 0.90 (0.01)
-- REM = 0.78 (0.07)
-- Wake = 0.87 (0.02)

First Combination

'LGBM__boosting_type': 'gbdt', 'LGBM__max_depth': 5, 'LGBM__min_child_samples': 100, 'LGBM__num_leaves': 21, 'LGBM__objective': 'binary'
- Accuracy = 0.85 (0.01)
- Kappa = 0.63 (0.02)
- Confusion matrix (mean; std)
[[0.85 0.05 0.1 ]
 [0.15 0.76 0.09]
 [0.12 0.02 0.86]]
[[0.01 0.01 0.01]
 [0.05 0.06 0.04]
 [0.04 0.01 0.04]]
- Stages recall
-- NREM = 0.85 (0.01)
-- REM = 0.76 (0.06)
-- Wake = 0.86 (0.04)

Second Combination

'LGBM__boosting_type': 'gbdt', 'LGBM__max_depth': 7, 'LGBM__min_child_samples': 100, 'LGBM__num_leaves': 30, 'LGBM__objective': 'binary'
- Accuracy = 0.88 (0.01)
- Kappa = 0.68 (0.03)
- Confusion matrix (mean; std)
[[0.88 0.04 0.08]
 [0.15 0.78 0.07]
 [0.11 0.02 0.87]]
[[0.01 0.01 0.01]
 [0.03 0.06 0.06]
 [0.02 0.01 0.02]]
- Stages recall
-- NREM = 0.88 (0.01)
-- REM = 0.78 (0.06)
-- Wake = 0.87 (0.02)

Third Combination

'LGBM__boosting_type': 'gbdt', 'LGBM__max_depth': 9, 'LGBM__min_child_samples': 100, 'LGBM__num_leaves': 250, 'LGBM__objective': 'binary'
- Accuracy = 0.89 (0.01)
- Kappa = 0.70 (0.03)
- Confusion matrix (mean; std)
[[0.9  0.03 0.07]
 [0.17 0.77 0.06]
 [0.12 0.01 0.87]]
[[0.01 0.01 0.01]
 [0.05 0.06 0.04]
 [0.02 0.01 0.02]]
- Stages recall
-- NREM = 0.90 (0.01)
-- REM = 0.77 (0.06)
-- Wake = 0.87 (0.02)

Fourth Combination

'LGBM__bagging_fraction': 0.82, 'LGBM__bagging_freq': 1, 'LGBM__boosting_type': 'rf', 'LGBM__max_depth': 9, 'LGBM__min_child_samples': 100, 'LGBM__num_leaves': 110, 'LGBM__objective': 'binary'
- Accuracy = 0.79 (0.02)
- Kappa = 0.53 (0.04)
- Confusion matrix (mean; std)
[[0.78 0.08 0.14]
 [0.11 0.78 0.12]
 [0.12 0.04 0.84]]
[[0.02 0.01 0.02]
 [0.06 0.07 0.07]
 [0.03 0.02 0.04]]
- Stages recall
-- NREM = 0.78 (0.02)
-- REM = 0.78 (0.07)
-- Wake = 0.84 (0.04)

XGBOOST
DEFAULT
               precision    recall  f1-score   support

        NREM       0.97      0.91      0.94       599
         Rem       0.71      0.78      0.74        37
        Wake       0.68      0.86      0.76       114

    accuracy                           0.90       750
   macro avg       0.79      0.85      0.81       750
weighted avg       0.91      0.90      0.90       750


Results...
- Accuracy = 0.90 (0.01)
- Kappa = 0.72 (0.04)
- Confusion matrix (mean; std)
[[0.92 0.02 0.06]
 [0.21 0.74 0.05]
 [0.14 0.01 0.84]]
[[0.01 0.   0.01]
 [0.05 0.06 0.04]
 [0.03 0.01 0.03]]
- Stages recall
-- NREM = 0.92 (0.01)
-- REM = 0.74 (0.06)
-- Wake = 0.84 (0.03)

FIRST COMBINATION

'XGB__booster': 'gbtree', 'XGB__gamma': 0, 'XGB__learning_rate': 0.3, 'XGB__max_delta_step': 0, 'XGB__max_depth': 10, 'XGB__min_child_weight': 0.5, 'XGB__sampling_method': 'uniform', 'XGB__subsample': 1, 'XGB__tree_method': 'auto'
               precision    recall  f1-score   support

        NREM       0.96      0.94      0.95       599
         Rem       0.68      0.76      0.72        37
        Wake       0.78      0.84      0.81       114

    accuracy                           0.92       750
   macro avg       0.81      0.85      0.83       750
weighted avg       0.92      0.92      0.92       750


Results...
- Accuracy = 0.91 (0.01)
- Kappa = 0.73 (0.03)
- Confusion matrix (mean; std)
[[0.93 0.02 0.05]
 [0.19 0.76 0.04]
 [0.18 0.01 0.81]]
[[0.01 0.   0.01]
 [0.09 0.09 0.04]
 [0.04 0.01 0.03]]
- Stages recall
-- NREM = 0.93 (0.01)
-- REM = 0.76 (0.09)
-- Wake = 0.81 (0.03)

SECOND COMBINATION

'XGB__booster': 'dart', 'XGB__normalize_type': 'forest', 'XGB__rate_drop': 0.3, 'XGB__sample_type': 'uniform'
               precision    recall  f1-score   support

        NREM       0.96      0.90      0.93       599
         Rem       0.49      0.76      0.60        37
        Wake       0.69      0.82      0.75       114

    accuracy                           0.88       750
   macro avg       0.71      0.82      0.76       750
weighted avg       0.90      0.88      0.89       750


Results...
- Accuracy = 0.87 (0.01)
- Kappa = 0.66 (0.03)
- Confusion matrix (mean; std)
[[0.88 0.04 0.08]
 [0.14 0.79 0.08]
 [0.11 0.03 0.86]]
[[0.01 0.01 0.01]
 [0.05 0.07 0.05]
 [0.03 0.02 0.03]]
- Stages recall
-- NREM = 0.88 (0.01)
-- REM = 0.79 (0.07)
-- Wake = 0.86 (0.03)

THIRD COMBINATION
{'XGB__alpha': 4, 'XGB__booster': 'gblinear', 'XGB__feature_selector': 'cyclic', 'XGB__lambda': 4, 'XGB__updater': 'coord_descent'}
               precision    recall  f1-score   support

        NREM       0.93      0.56      0.70       599
         Rem       0.10      0.59      0.17        37
        Wake       0.31      0.46      0.37       114

    accuracy                           0.55       750
   macro avg       0.45      0.54      0.41       750
weighted avg       0.79      0.55      0.62       750


Results...
- Accuracy = 0.59 (0.08)
- Kappa = 0.20 (0.07)
- Confusion matrix (mean; std)
[[0.61 0.21 0.18]
 [0.24 0.52 0.24]
 [0.28 0.22 0.51]]
[[0.13 0.07 0.06]
 [0.26 0.19 0.11]
 [0.24 0.09 0.18]]
- Stages recall
-- NREM = 0.61 (0.13)
-- REM = 0.52 (0.19)
-- Wake = 0.51 (0.18)

FOURTH COMBINATION

  {'XGB__alpha': 2, 'XGB__booster': 'gblinear', 'XGB__feature_selector': 'cyclic', 'XGB__lambda': 0, 'XGB__updater': 'shotgun'}
               precision    recall  f1-score   support

        NREM       0.93      0.56      0.70       599
         Rem       0.11      0.62      0.18        37
        Wake       0.30      0.46      0.36       114

    accuracy                           0.55       750
   macro avg       0.44      0.55      0.41       750
weighted avg       0.79      0.55      0.62       750


Results...
- Accuracy = 0.59 (0.08)
- Kappa = 0.20 (0.07)
- Confusion matrix (mean; std)
[[0.61 0.22 0.18]
 [0.23 0.52 0.24]
 [0.28 0.22 0.5 ]]
[[0.13 0.07 0.06]
 [0.26 0.19 0.11]
 [0.24 0.09 0.18]]
- Stages recall
-- NREM = 0.61 (0.13)
-- REM = 0.52 (0.19)
-- Wake = 0.50 (0.18)

Fifth Combination

'XGB__colsample_bytree': 1.0, 'XGB__gamma': 0.5, 'XGB__max_depth': 5, 'XGB__min_child_weight': 1, 'XGB__subsample': 0.8
               precision    recall  f1-score   support

        NREM       0.97      0.92      0.95       599
         Rem       0.57      0.73      0.64        37
        Wake       0.76      0.87      0.81       114

    accuracy                           0.91       750
   macro avg       0.77      0.84      0.80       750
weighted avg       0.92      0.91      0.91       750


Results...
- Accuracy = 0.90 (0.01)
- Kappa = 0.71 (0.03)
- Confusion matrix (mean; std)
[[0.91 0.03 0.06]
 [0.19 0.75 0.05]
 [0.13 0.02 0.85]]
[[0.01 0.01 0.01]
 [0.06 0.07 0.04]
 [0.04 0.01 0.03]]
- Stages recall
-- NREM = 0.91 (0.01)
-- REM = 0.75 (0.07)
-- Wake = 0.85 (0.03)

LDA

'solver' : 'svd'

Results...
- Accuracy = 0.49 (0.02)
- Kappa = 0.17 (0.01)
- Confusion matrix (mean; std)
[[0.48 0.34 0.19]
 [0.14 0.55 0.32]
 [0.18 0.28 0.55]]
[[0.03 0.03 0.02]
 [0.04 0.07 0.04]
 [0.04 0.04 0.05]]
- Stages recall
-- NREM = 0.48 (0.03)
-- REM = 0.55 (0.07)
-- Wake = 0.55 (0.05)

RF

'max_depth': 100, 'max_features': 'sqrt', 'n_estimators': 1000

Results...
- Accuracy = 0.91 (0.01)
- Kappa = 0.74 (0.02)
- Confusion matrix (mean; std)
[[0.93 0.02 0.05]
 [0.2  0.76 0.04]
 [0.15 0.01 0.84]]
[[0.01 0.   0.01]
 [0.05 0.07 0.03]
 [0.03 0.01 0.03]]
- Stages recall
-- NREM = 0.93 (0.01)
-- REM = 0.76 (0.07)
-- Wake = 0.84 (0.03)

KNN

'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'

Results...
- Accuracy = 0.90 (0.01)
- Kappa = 0.71 (0.04)
- Confusion matrix (mean; std)
[[0.92 0.03 0.06]
 [0.14 0.82 0.04]
 [0.16 0.02 0.82]]
[[0.01 0.01 0.01]
 [0.05 0.06 0.02]
 [0.03 0.02 0.03]]
- Stages recall
-- NREM = 0.92 (0.01)
-- REM = 0.82 (0.06)
-- Wake = 0.82 (0.03)

SVC

'C': 100, 'gamma': 1, 'kernel': 'rbf', 'class_weight': None

Results...
- Accuracy = 0.85 (0.01)
- Kappa = 0.62 (0.02)
- Confusion matrix (mean; std)
[[0.86 0.07 0.08]
 [0.16 0.8  0.04]
 [0.15 0.03 0.83]]
[[0.01 0.01 0.02]
 [0.06 0.06 0.02]
 [0.03 0.01 0.03]]
- Stages recall
-- NREM = 0.86 (0.01)
-- REM = 0.80 (0.06)
-- Wake = 0.83 (0.03)